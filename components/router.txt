# from openai import OpenAI
# from dotenv import load_dotenv
# from os import getenv

# load_dotenv()
# gets API Key from environment variable OPENAI_API_KEY
# client = OpenAI(
#     base_url="https://openrouter.ai/api/v1",
#     api_key=getenv("OPENROUTER_API_KEY"),
# )

# completion = client.chat.completions.create(
#     # extra_headers={
#     #   "HTTP-Referer": $YOUR_SITE_URL, # Optional, for including your app on openrouter.ai rankings.
#     #   "X-Title": $YOUR_APP_NAME, # Optional. Shows in rankings on openrouter.ai.
#     # },
#     model="meta-llama/llama-3.1-8b-instruct:free",
#     messages=[
#         {
#           "role": "system",
#           "content": "Tell me a joke",
#         },
#     ],
# )
# print(completion.choices[0].message.content)

# from flask import Flask, request, Response, jsonify
# from openai import OpenAI
# from os import getenv

# app = Flask(__name__)

# # Get API Key from environment variable
# client = OpenAI(
#     base_url="https://openrouter.ai/api/v1",
#     api_key='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'
# )

# # System prompt for the AI, providing guidelines on how to respond to users
# system_prompt = "Use your own system prompt here"

# @app.route('/api/chat', methods=['POST'])
# def chat():
#     # Get JSON data from the request
#     data = request.json

#     # Create a chat completion request to the OpenRouter API
#     try:
#         completion = client.chat.completions.create(
#             model="meta-llama/llama-3.1-8b-instruct:free",  # Specify the model to use
#             messages=[{"role": "system", "content": system_prompt}] + data,  # Include the system prompt and user messages
#             stream=True  # Enable streaming responses
#         )
        
#         # Define a generator to stream the response
#         def generate():
#             try:
#                 for chunk in completion:
#                     content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
#                     print(content)
#                     if content:
#                         yield content
#             except Exception as e:
#                 yield f"Error: {str(e)}"

#         # Return a streaming response
#         return Response(generate(), content_type='text/plain')

#     except Exception as e:
#         return jsonify({"error": str(e)}), 500

# if __name__ == '__main__':
#     app.run(debug=True)